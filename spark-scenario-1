### Data read and processing#########
source_rdd = spark.sparkContext.parallelize([
    (1, "A"),
    (2, "B"),
    (3, "C"),
    (4, "D")
],1)

target_rdd = spark.sparkContext.parallelize([
    (1, "A"),
    (2, "B"),
    (4, "X"),
    (5, "F")
],2)
# Convert RDDs to DataFrames using toDF()
df1 = source_rdd.toDF(["id", "name"])
df2 = target_rdd.toDF(["id", "name1"])

# Show the DataFrames
df1.show()
df2.show()
print("===FULL JOIN====")
fulljoin = df1.join (df2, ["id"] , "full")
fulljoin.show()
match = fulljoin.withColumn("comment",expr("""
                                  case
                                  when  name=name1  then 'match'
                                  else 'mismatch'
                                  end
                        """))
match.show()
filterdf = match.filter(" comment ='mismatch' ")
filterdf.show()
finaldf = filterdf.withColumn("comment",expr("""
                                       case
                                       when name1 is null then 'New in Source'
                                       when name  is null then 'New in Target'
                                       else comment
                                       end
                                    """))
finaldf.show()
finalfinaldf = finaldf.drop("name","name1")
finalfinaldf.show()
